# å¿«é€Ÿå¼€å§‹æŒ‡å—

æ¬¢è¿ä½¿ç”¨å¼€æºå¤§æ¨¡å‹éƒ¨ç½²æ–¹æ¡ˆï¼æœ¬æŒ‡å—å°†å¸®åŠ©ä½ åœ¨ 5 åˆ†é’Ÿå†…éƒ¨ç½²ä¸€ä¸ªæ”¯æŒ Claude Code å·¥å…·è°ƒç”¨çš„æœ¬åœ°å¤§æ¨¡å‹ã€‚


## å‰ææ¡ä»¶

- Ubuntu 18.04 æˆ–æ›´é«˜ç‰ˆæœ¬
- è‡³å°‘ 32GB å†…å­˜ï¼ˆæ¨è 64GB+ï¼‰
- Docker å’Œ Docker Compose å·²å®‰è£…


## å¿«é€Ÿéƒ¨ç½²ï¼ˆ3 æ­¥ï¼‰

### 1. å…‹éš†ä»“åº“

```bash
git clone https://github.com/githubstudycloud/gi009.git
cd gi009
```

### 2. å¯åŠ¨æœåŠ¡

**æ–¹å¼Aï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰**

```bash
chmod +x start.sh
./start.sh
```

è„šæœ¬ä¼šå¼•å¯¼ä½ é€‰æ‹©åˆé€‚çš„æ¨¡å‹å¹¶è‡ªåŠ¨å¯åŠ¨æœåŠ¡ã€‚

**æ–¹å¼Bï¼šæ‰‹åŠ¨å¯åŠ¨**

```bash
# æ–¹æ¡ˆ1: Qwen2.5-Coder-32B (æ¨è)
docker-compose up -d

# æ–¹æ¡ˆ2: DeepSeek-Coder-V2-Lite (çœå†…å­˜)
docker-compose -f docker-compose-deepseek.yml up -d

# æ–¹æ¡ˆ3: Qwen2.5-72B (æ›´å¼ºæ€§èƒ½)
docker-compose -f docker-compose-qwen72b.yml up -d
```

### 3. ç­‰å¾…æ¨¡å‹åŠ è½½

é¦–æ¬¡å¯åŠ¨ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œéœ€è¦ 15-30 åˆ†é’Ÿã€‚æŸ¥çœ‹è¿›åº¦ï¼š

```bash
docker-compose logs -f
```

å½“çœ‹åˆ° "Application startup complete" æ—¶ï¼Œè¡¨ç¤ºæœåŠ¡å·²å°±ç»ªã€‚


## éªŒè¯éƒ¨ç½²

### æµ‹è¯• API

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# æµ‹è¯•å¯¹è¯
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5-coder-32b-instruct",
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "max_tokens": 100
  }'
```

### ä½¿ç”¨æµ‹è¯•è„šæœ¬

```bash
chmod +x test-api.sh
./test-api.sh
```

### è¿è¡Œæ€§èƒ½æµ‹è¯•

```bash
pip3 install aiohttp
python3 benchmark.py
```


## æ¨¡å‹é€‰æ‹©æŒ‡å—

### æ¨èé…ç½®

| å†…å­˜å¤§å° | æ¨èæ¨¡å‹ | ç‰¹ç‚¹ |
|---------|---------|------|
| 32-50GB | DeepSeek-Coder-V2-Lite | çœèµ„æºï¼Œä»£ç èƒ½åŠ›å¼º |
| 50-70GB | **Qwen2.5-Coder-32B** | **æœ€ä½³å¹³è¡¡ï¼ˆæ¨èï¼‰** |
| 70-100GB | Qwen2.5-72B | æœ€å¼ºæ€§èƒ½ |

### è¯¦ç»†å¯¹æ¯”

#### Qwen2.5-Coder-32B â­ æ¨è

```bash
docker-compose up -d
```

- **å†…å­˜å ç”¨**: 40-50GB
- **ä»£ç èƒ½åŠ›**: â­â­â­â­â­
- **ä¸­æ–‡æ”¯æŒ**: â­â­â­â­â­
- **å“åº”é€Ÿåº¦**: â­â­â­â­
- **å·¥å…·è°ƒç”¨**: âœ… åŸç”Ÿæ”¯æŒ
- **é€‚ç”¨åœºæ™¯**: ä»£ç ç”Ÿæˆã€API è°ƒç”¨ã€æ–‡æ¡£ç¼–å†™

#### DeepSeek-Coder-V2-Lite

```bash
docker-compose -f docker-compose-deepseek.yml up -d
```

- **å†…å­˜å ç”¨**: 20-30GB
- **ä»£ç èƒ½åŠ›**: â­â­â­â­
- **ä¸­æ–‡æ”¯æŒ**: â­â­â­â­
- **å“åº”é€Ÿåº¦**: â­â­â­â­â­
- **å·¥å…·è°ƒç”¨**: âœ… æ”¯æŒ
- **é€‚ç”¨åœºæ™¯**: èµ„æºå—é™ç¯å¢ƒã€å¿«é€Ÿå“åº”

#### Qwen2.5-72B

```bash
docker-compose -f docker-compose-qwen72b.yml up -d
```

- **å†…å­˜å ç”¨**: 80-100GB
- **ä»£ç èƒ½åŠ›**: â­â­â­â­â­
- **ä¸­æ–‡æ”¯æŒ**: â­â­â­â­â­
- **å“åº”é€Ÿåº¦**: â­â­â­
- **å·¥å…·è°ƒç”¨**: âœ… åŸç”Ÿæ”¯æŒ
- **é€‚ç”¨åœºæ™¯**: å¤æ‚æ¨ç†ã€é«˜è´¨é‡ä»£ç ç”Ÿæˆ


## é›†æˆ Claude Code

### æ–¹æ³• 1ï¼šä½¿ç”¨ MCP æœåŠ¡å™¨

```bash
# å®‰è£… MCP OpenAI æœåŠ¡å™¨
npm install -g @modelcontextprotocol/server-openai

# é…ç½® Claude Code
# ç¼–è¾‘ ~/.config/claude-code/config.json
```

æ·»åŠ é…ç½®ï¼š

```json
{
  "mcpServers": {
    "local-llm": {
      "command": "mcp-server-openai",
      "env": {
        "OPENAI_API_KEY": "dummy",
        "OPENAI_BASE_URL": "http://localhost:8000/v1"
      }
    }
  }
}
```

è¯¦ç»†é›†æˆæŒ‡å—è¯·æŸ¥çœ‹ [CLAUDE_CODE_INTEGRATION.md](./CLAUDE_CODE_INTEGRATION.md)


## å¸¸ç”¨å‘½ä»¤

### æŸ¥çœ‹æ—¥å¿—

```bash
docker-compose logs -f
```

### æŸ¥çœ‹çŠ¶æ€

```bash
docker-compose ps
docker stats
```

### é‡å¯æœåŠ¡

```bash
docker-compose restart
```

### åœæ­¢æœåŠ¡

```bash
docker-compose down
```

### æ›´æ–°æœåŠ¡

```bash
docker-compose pull
docker-compose up -d
```


## æ€§èƒ½è°ƒä¼˜

### è°ƒæ•´å¹¶å‘æ•°

ç¼–è¾‘ `docker-compose.yml`ï¼š

```yaml
command: >
  ...
  --max-num-seqs 10    # 10ä¸ªç”¨æˆ·å»ºè®®è®¾ä¸º10-20
```

### è°ƒæ•´ä¸Šä¸‹æ–‡é•¿åº¦

```yaml
--max-model-len 8192   # å‡å°‘å¯é™ä½å†…å­˜å ç”¨
```

### æŸ¥çœ‹èµ„æºä½¿ç”¨

```bash
docker stats --no-stream
```


## æ•…éšœæ’æŸ¥

### é—®é¢˜ 1ï¼šå†…å­˜ä¸è¶³

**ç—‡çŠ¶**ï¼šå®¹å™¨é¢‘ç¹é‡å¯

**è§£å†³**ï¼š
```bash
# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
docker-compose -f docker-compose-deepseek.yml up -d

# æˆ–å‡å°‘å¹¶å‘æ•°
# ç¼–è¾‘ docker-compose.ymlï¼Œå‡å°‘ --max-num-seqs
```

### é—®é¢˜ 2ï¼šæ¨¡å‹ä¸‹è½½æ…¢

**ç—‡çŠ¶**ï¼šé•¿æ—¶é—´åœåœ¨ä¸‹è½½ç•Œé¢

**è§£å†³**ï¼šä½¿ç”¨ HF é•œåƒï¼ˆå·²åœ¨é…ç½®ä¸­å¯ç”¨ï¼‰
```yaml
environment:
  - HF_ENDPOINT=https://hf-mirror.com
```

### é—®é¢˜ 3ï¼šç«¯å£è¢«å ç”¨

**ç—‡çŠ¶**ï¼š`address already in use`

**è§£å†³**ï¼šä¿®æ”¹ç«¯å£
```yaml
ports:
  - "8001:8000"  # æ”¹ä¸ºå…¶ä»–ç«¯å£
```

### é—®é¢˜ 4ï¼šAPI æ— å“åº”

**æ£€æŸ¥**ï¼š
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs --tail=50

# æµ‹è¯•å¥åº·æ£€æŸ¥
curl http://localhost:8000/health
```


## ä½¿ç”¨ç¤ºä¾‹

### Python ç¤ºä¾‹

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"
)

response = client.chat.completions.create(
    model="qwen2.5-coder-32b-instruct",
    messages=[
        {"role": "user", "content": "å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åº"}
    ]
)

print(response.choices[0].message.content)
```

### curl ç¤ºä¾‹

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5-coder-32b-instruct",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹"},
      {"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯Docker"}
    ],
    "temperature": 0.7,
    "max_tokens": 1000
  }'
```

### Function Calling ç¤ºä¾‹

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5-coder-32b-instruct",
    "messages": [
      {"role": "user", "content": "å¸®æˆ‘æœç´¢å¿«é€Ÿæ’åºçš„ä»£ç "}
    ],
    "tools": [{
      "type": "function",
      "function": {
        "name": "search_code",
        "description": "æœç´¢ä»£ç åº“",
        "parameters": {
          "type": "object",
          "properties": {
            "query": {"type": "string"}
          },
          "required": ["query"]
        }
      }
    }]
  }'
```


## ä¸‹ä¸€æ­¥

- ğŸ“– [å®Œæ•´éƒ¨ç½²æŒ‡å—](./DEPLOYMENT.md)
- ğŸ”Œ [API ä½¿ç”¨æ–‡æ¡£](./API.md)
- ğŸ¤– [Claude Code é›†æˆ](./CLAUDE_CODE_INTEGRATION.md)
- âš¡ [æ€§èƒ½æµ‹è¯•](./benchmark.py)


## è·å–å¸®åŠ©

- æŸ¥çœ‹æ—¥å¿—ï¼š`docker-compose logs -f`
- æŸ¥çœ‹æ–‡æ¡£ï¼šæµè§ˆæœ¬ä»“åº“çš„ Markdown æ–‡ä»¶
- æäº¤é—®é¢˜ï¼šhttps://github.com/githubstudycloud/gi009/issues


## å¸¸è§é—®é¢˜

**Q: éœ€è¦ GPU å—ï¼Ÿ**
A: ä¸éœ€è¦ï¼Œæœ¬æ–¹æ¡ˆä½¿ç”¨ CPU æ¨ç†ã€‚

**Q: èƒ½æ”¯æŒå¤šå°‘ç”¨æˆ·ï¼Ÿ**
A: æ¨èé…ç½®å¯æ”¯æŒ 5-10 ä¸ªç”¨æˆ·åŒæ—¶ä½¿ç”¨ã€‚

**Q: å“åº”é€Ÿåº¦å¦‚ä½•ï¼Ÿ**
A: CPU æ¨ç†é€Ÿåº¦çº¦ä¸º 5-15 tokens/ç§’ï¼Œå–å†³äº CPU æ€§èƒ½ã€‚

**Q: æ”¯æŒå“ªäº›è¯­è¨€ï¼Ÿ**
A: æ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰å¤šç§è¯­è¨€ï¼Œä¸­æ–‡æ”¯æŒç‰¹åˆ«å¥½ã€‚

**Q: å¦‚ä½•æ›´æ–°æ¨¡å‹ï¼Ÿ**
A: `docker-compose pull && docker-compose up -d`


## è®¸å¯è¯

MIT License
